{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Feature Processing\n",
    "\n",
    "In this example we are going to augment a dataset with NLP derived text features using the Amazon Comprehend APIs.\n",
    "\n",
    "Note: That to execute this Notebook your role will need the following policy attached: ComprehendFullAccess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-06-18 05:31:21--  https://www.dt.fee.unicamp.br/~tiago/smsspamcollection/smsspamcollection.zip\n",
      "Resolving www.dt.fee.unicamp.br (www.dt.fee.unicamp.br)... 143.106.12.20\n",
      "Connecting to www.dt.fee.unicamp.br (www.dt.fee.unicamp.br)|143.106.12.20|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 210521 (206K) [application/zip]\n",
      "Saving to: ‘data/smsspamcollection.zip’\n",
      "\n",
      "data/smsspamcollect 100%[===================>] 205.59K  80.1KB/s    in 2.6s    \n",
      "\n",
      "2021-06-18 05:31:26 (80.1 KB/s) - ‘data/smsspamcollection.zip’ saved [210521/210521]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -O data/smsspamcollection.zip https://www.dt.fee.unicamp.br/~tiago/smsspamcollection/smsspamcollection.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1472\n",
      "drwxr-xr-x 2 root root   6144 Jun 18 05:42 .\n",
      "drwxr-xr-x 7 root root   6144 Jun 22 03:10 ..\n",
      "-rwxr-xr-x 1 root root 477907 Mar 16  2011 SMSSpamCollection.txt\n",
      "-rw-r--r-- 1 root root   5868 Apr 18  2011 readme\n",
      "-rw-r--r-- 1 root root 129103 Jun 18 04:08 score.csv\n",
      "-rw-r--r-- 1 root root 210521 Jul  3  2015 smsspamcollection.zip\n",
      "-rw-r--r-- 1 root root 132691 Jun 16 12:22 test.csv\n",
      "-rw-r--r-- 1 root root 396174 Jun 16 12:22 train.csv\n",
      "-rw-r--r-- 1 root root 132688 Jun 16 12:22 validation.csv\n"
     ]
    }
   ],
   "source": [
    "!ls -la data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  data/smsspamcollection.zip\n",
      "  inflating: data/readme             \n",
      "  inflating: data/SMSSpamCollection.txt  \n"
     ]
    }
   ],
   "source": [
    "!unzip data/smsspamcollection.zip -d data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ham\tGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
      "ham\tOk lar... Joking wif u oni...\n",
      "spam\tFree entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n",
      "ham\tU dun say so early hor... U c already then say...\n",
      "ham\tNah I don't think he goes to usf, he lives around here though\n",
      "spam\tFreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, £1.50 to rcv\n",
      "ham\tEven my brother is not like to speak with me. They treat me like aids patent.\n",
      "ham\tAs per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune\n",
      "spam\tWINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.\n",
      "spam\tHad your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030\n"
     ]
    }
   ],
   "source": [
    "!head data/SMSSpamCollection.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/SMSSpamCollection.txt\", delimiter=\"\\t\", header=None)\n",
    "df.columns = [\"class\",\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  class                                               text\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "boto_session = boto3.Session()\n",
    "region = boto_session.region_name\n",
    "comprehend = boto3.client('comprehend', region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehend API for single block of text\n",
    "\n",
    "In these examples we use comprehend to extract features from a single block of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- phrases ---------\n",
      "a great day\n",
      "my equestrian knowledge\n",
      "good use\n",
      "work\n",
      "------- entity : entity type ---------\n",
      "#AWS : ORGANIZATION\n",
      "#BePeculiar : TITLE\n",
      "------- sentiment ---------\n",
      "POSITIVE\n"
     ]
    }
   ],
   "source": [
    "sample_tweet=\"It’s always a great day when I can randomly put my equestrian knowledge to good use at work! #AWS #BePeculiar\"   \n",
    "\n",
    "# Key phrases\n",
    "phrases = comprehend.detect_key_phrases(Text=sample_tweet, LanguageCode='en')\n",
    "\n",
    "# Entities\n",
    "entities = comprehend.detect_entities(Text=sample_tweet, LanguageCode='en')\n",
    "\n",
    "#Sentiments\n",
    "sentiments = comprehend.detect_sentiment(Text=sample_tweet, LanguageCode='en')\n",
    "\n",
    "\n",
    "# Print the phrases:\n",
    "print('------- phrases ---------')\n",
    "for i in range(0, len(phrases['KeyPhrases'])):\n",
    "    print((phrases['KeyPhrases'][i]['Text']))\n",
    "    \n",
    "\n",
    "# Print the entities with entitity type:\n",
    "print('------- entity : entity type ---------')\n",
    "for i in range(0, len(entities['Entities'])):\n",
    "    print(entities['Entities'][i]['Text'] + ' : ' + entities['Entities'][i]['Type'] )\n",
    "    \n",
    "# Print the sentiment:\n",
    "print('------- sentiment ---------')\n",
    "print(sentiments['Sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language detection\n",
    "\n",
    "It is also possible to first detect the language and then use that in subsequent comprehend API calls for additinal NLP features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lan = comprehend.detect_dominant_language(Text=sample_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = lan['Languages'][0]['LanguageCode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'en'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Jobs\n",
    "\n",
    "Comprehend has separate APIs to score batches of text blocks\n",
    "\n",
    "https://docs.aws.amazon.com/comprehend/latest/dg/API_BatchDetectSentiment.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rez = comprehend.batch_detect_sentiment(TextList=['happy days', 'not feeling good'], LanguageCode='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResultList': [{'Index': 0,\n",
       "   'Sentiment': 'POSITIVE',\n",
       "   'SentimentScore': {'Positive': 0.9528746604919434,\n",
       "    'Negative': 0.0026089251041412354,\n",
       "    'Neutral': 0.037291351705789566,\n",
       "    'Mixed': 0.007225051987916231}},\n",
       "  {'Index': 1,\n",
       "   'Sentiment': 'NEGATIVE',\n",
       "   'SentimentScore': {'Positive': 0.0058039030991494656,\n",
       "    'Negative': 0.956430196762085,\n",
       "    'Neutral': 0.014397521503269672,\n",
       "    'Mixed': 0.023368341848254204}}],\n",
       " 'ErrorList': [],\n",
       " 'ResponseMetadata': {'RequestId': 'b65e109e-8f74-48a9-b2fa-276e3c822935',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'b65e109e-8f74-48a9-b2fa-276e3c822935',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '384',\n",
       "   'date': 'Tue, 22 Jun 2021 03:55:43 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Feature Engineering Pipeline\n",
    "\n",
    "Now that we understand the fundamentals of the Comprehend APIs, lets build a function that uses it to add features to a dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Comprehend batching scoring API - comprehend.batch_detect_sentiment -\n",
    "# will only takes a max of 25 documents\n",
    "# So we build a simple wrapper to score larger numbers of documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "flatten = lambda t: [item for sublist in t for item in sublist]\n",
    "\n",
    "def batch_sentiment(text_list, lan='en'):\n",
    "    max_len = 25\n",
    "    list_len = len(text_list)\n",
    "    chunks = math.floor(list_len/max_len) + 1\n",
    "    rez = []\n",
    "    for index in range(0,chunks):\n",
    "        start_index = index * max_len\n",
    "        sublist = text_list[(start_index):(start_index+max_len)]\n",
    "        xents = comprehend.batch_detect_sentiment(TextList=sublist, LanguageCode=lan)\n",
    "        rez.append(xents['ResultList'])\n",
    "    return flatten(rez)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data_frame(df, col, lan='en'):\n",
    "    rez = df.copy()\n",
    "    str_vals = df[col].values.tolist()\n",
    "    sent = batch_sentiment( str_vals, lan )\n",
    "    posvals = [ x['SentimentScore']['Positive'] for x in sent]\n",
    "    negvals = [ x['SentimentScore']['Negative'] for x in sent]\n",
    "    mixvals = [ x['SentimentScore']['Mixed'] for x in sent]\n",
    "    rez[\"Sentiment_positive\"] = posvals\n",
    "    rez[\"Sentiment_negative\"] = negvals\n",
    "    rez[\"Sentiment_mixed\"] = mixvals\n",
    "    return rez\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented = augment_data_frame(df, 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>text</th>\n",
       "      <th>Sentiment_positive</th>\n",
       "      <th>Sentiment_negative</th>\n",
       "      <th>Sentiment_mixed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0.428451</td>\n",
       "      <td>0.233218</td>\n",
       "      <td>0.168349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0.009710</td>\n",
       "      <td>0.519999</td>\n",
       "      <td>0.082917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>0.007450</td>\n",
       "      <td>0.000403</td>\n",
       "      <td>0.000030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0.056410</td>\n",
       "      <td>0.084197</td>\n",
       "      <td>0.005699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0.029031</td>\n",
       "      <td>0.247017</td>\n",
       "      <td>0.008144</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  class                                               text  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...   \n",
       "1   ham                      Ok lar... Joking wif u oni...   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3   ham  U dun say so early hor... U c already then say...   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "   Sentiment_positive  Sentiment_negative  Sentiment_mixed  \n",
       "0            0.428451            0.233218         0.168349  \n",
       "1            0.009710            0.519999         0.082917  \n",
       "2            0.007450            0.000403         0.000030  \n",
       "3            0.056410            0.084197         0.005699  \n",
       "4            0.029031            0.247017         0.008144  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repeat\n",
    "\n",
    "The same process can be repeated for the other Comprehend APIs to augement the text with additional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:ap-northeast-2:806072073708:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
